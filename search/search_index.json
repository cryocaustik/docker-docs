{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Docker Docs Documentation on getting started with containerizing applications/services in docker, using docker-compose. Documentation is broken down into the sections below: Project Structure Structuring project to prepare for docker. Dockerfile Core Docker configuration for building custom containers. Docker Compose Simplified YAML Configuration for Docker stack. Docker Ignore Area to define things that Docker should not touch. Environment File Simply and securely defining sensitive variables for containers.","title":"Introduction"},{"location":"#docker-docs","text":"Documentation on getting started with containerizing applications/services in docker, using docker-compose. Documentation is broken down into the sections below:","title":"Docker Docs"},{"location":"#project-structure","text":"Structuring project to prepare for docker.","title":"Project Structure"},{"location":"#dockerfile","text":"Core Docker configuration for building custom containers.","title":"Dockerfile"},{"location":"#docker-compose","text":"Simplified YAML Configuration for Docker stack.","title":"Docker Compose"},{"location":"#docker-ignore","text":"Area to define things that Docker should not touch.","title":"Docker Ignore"},{"location":"#environment-file","text":"Simply and securely defining sensitive variables for containers.","title":"Environment File"},{"location":"docker_compose/","text":"Docker Compose Docker Compose (aka docker-compose) is a simplified method to defining and configuring a whole Docker stack of services, allowing for quick configuration of not just single containers, but multiple containers/services, and their interactivity between each other. While all functionality of docker-compose can be re-created using only a Dockerfile and manual commands or scripted shell script, a docker-compose file allows quick and simple configuration through a single yaml file per environment (e.g. dev, test, prod). Below we will go over the sections of docker-compose and how they're used, with a final completed sample at the end. Offial documentation on Docker Compose can be found on Docker's Docs site . File Naming Docker Compose by default, will always look for a docker-compose.yml file in the directory where it's commands are ran; if a standard file is found, Docker Compose will use that for executing given commands. This functionality enables users the option of either: Define a standard file ( docker-compose.yml ) for a single environment and be Docker Compose ready for all commands or, Define multiple environments using variations of: docker-compose.yml for development docker-compose.test.yaml for testing docker-compose.prod.yaml for production By utilizing multiple configuration files for each environment, we are able to define custom settings (e.g. no web server needed during development), relevant variables (e.g. use test dataabse for testing), and services/containers (e.g. running a buidl/migration step for production). Example: Wanting to setup both, a development and production configuration separately, we will create: docker-compose.yml for development docker-compose.prod.yml for production Version The version keyword tells docker-compose what version of configurations we will be using. This helps with issues of using configurations, keywords, etc. that may have been used in earlier versions, but have since changed. The first line of our docker-compose file should always be the version keyword. Example: If we wanted to use version 3.7 (latest at the time of documentation), we would use: version : \"3.7\" Services Services with ** require additional configuration variables defined on this page Certain service parameters like Persistent Volumes and Shared Netowrks require additional configuration outside of the Services section. Services with this identifier will have their own section in this document. The services section is where we define all the containers that we want Docker to create and their configurations/relations. Each entry in the services section can have varying options and environment variables, which will be best defined on the core images' Dockerhub page. Below is a matrix of popular parameters for services and wether they're required for base functionality: Parameter Required Default Value Description container_name N random string Custom name for container; extremely useful to identify containers that belong to a stack. image Y Image name to be used for container. This is replaced by build when using Dockerfile to build a custom image. build Y Path to Dockerfile . This is replaced by image when using pre-built imaged from Dockerhub restart N never Define if container should attempt to restart on fail or docker boot (e.g. system restart) environment N Environment variables that should be passed into the container. ports N Port mapping from local system to container volumes N Docker Volume and local filesystem mapping into the container. Mapping a local directory to the container (e.g. ./src ) will ensure the files are always in-sync, in both directions, even while the container is running. networks N Shared network mapping between containers.** command N Custom command to be ran on container start/boot.** If we wanted to define: Node service that builds from our Dockerfile MySQL database from a pre-built image in Dockerhub Allow containers to communicate between each other Ensure our DB data persists even when containers are shutdown/destroyed Expose our Node service to our local system on port 80 we would use: version : \"3.7\" services : # our node application node_app : container_name : node_web # custom name for the container build : . # use Dockerfile located project root restart : unless-stopped # always restart, unless manually stopped environment : # define environment variables NODE_ENVIRONMENT : dev ports : # map ports from local:container - 80:8000 volumes : # map our source code into container workdir - ./src:/code networks : # use shared network between containers - node_db_network # mysql db for our node application node_db : container_name : node_db # custom name for the container image : mysql:latest # name:version of pre-built image restart : unless-stopped # always restart, unless manually stopped environment : # define environment variables MYSQL_ROOT_PASSWORD : root_pwd MYSQL_DATABASE : app_db MYSQL_USER : db_admin MYSQL_PASSWORD : db_admin_pwd volumes : # map persistent volume to DB store - node_db_volume:/var/lib/mysql networks : # use shared network between containers - node_db_network Volumes In docker-compose, volumes can be used to: Create simple mappings of local code/config directories into the container, Map local directories to sync and retain data from within the container, Define Persistent Volumes that are named and kept even when containers are shutdown/recreated. Simple Volumes In our examples we use a simple mapping to map our ./src directory into the container /code work directory, which ensures our source code is always in-sync, allowing us to make live edits to code while the containers are running. volumes : - ./src:/code Persistent Volumes For our database however, we do not have pre-existing data that we would want MySQL to use; instead, we would like to persist our data that is created once our application and database are up and running in Docker. To achieve this, we will map out network similar to a Simple Volume, but add a separate configuration outside of the Services section: # our usual services definition, where we # map our persistent volume to the container services : node_db : ... volumes : - node_db_network # persistent volume definition volumes : node_db_network : Networks While we do define and expose ports on the individual containers to our local system, there are cases where we want certain containers to communicate with each other, and only each other. To achieve this, we create named networks and then define which containers have access to them. If we wanted to let our node_web container communicate with the database in node_db container, without exposing it to our local system, we would use: # our usual services definition, where we map our # named network to the containers that are allowed to use it services : node_web : ... networks : - node_db_network node_db : ... networks : - node_db_network # named network definition networks : node_db_network : Completed Example version : \"3.7\" services : # our node application node_app : container_name : node_web # custom name for the container build : . # use Dockerfile located project root restart : unless-stopped # always restart, unless manually stopped environment : # define environment variables NODE_ENVIRONMENT : dev ports : # map ports from local:container - 80:8000 volumes : # map our source code into container workdir - ./src:/code networks : # use shared network between containers - node_db_network # mysql db for our node application node_db : container_name : node_db # custom name for the container image : mysql:latest # name:version of pre-built image restart : unless-stopped # always restart, unless manually stopped environment : # define environment variables MYSQL_ROOT_PASSWORD : root_pwd MYSQL_DATABASE : app_db MYSQL_USER : db_admin MYSQL_PASSWORD : db_admin_pwd volumes : # map persistent volume to DB store - node_db_volume:/var/lib/mysql networks : # use shared network between containers - node_db_network # persistent volume definition volumes : node_db_network : # named network definition networks : node_db_network :","title":"Docker Compose"},{"location":"docker_compose/#docker-compose","text":"Docker Compose (aka docker-compose) is a simplified method to defining and configuring a whole Docker stack of services, allowing for quick configuration of not just single containers, but multiple containers/services, and their interactivity between each other. While all functionality of docker-compose can be re-created using only a Dockerfile and manual commands or scripted shell script, a docker-compose file allows quick and simple configuration through a single yaml file per environment (e.g. dev, test, prod). Below we will go over the sections of docker-compose and how they're used, with a final completed sample at the end. Offial documentation on Docker Compose can be found on Docker's Docs site .","title":"Docker Compose"},{"location":"docker_compose/#file-naming","text":"Docker Compose by default, will always look for a docker-compose.yml file in the directory where it's commands are ran; if a standard file is found, Docker Compose will use that for executing given commands. This functionality enables users the option of either: Define a standard file ( docker-compose.yml ) for a single environment and be Docker Compose ready for all commands or, Define multiple environments using variations of: docker-compose.yml for development docker-compose.test.yaml for testing docker-compose.prod.yaml for production By utilizing multiple configuration files for each environment, we are able to define custom settings (e.g. no web server needed during development), relevant variables (e.g. use test dataabse for testing), and services/containers (e.g. running a buidl/migration step for production). Example: Wanting to setup both, a development and production configuration separately, we will create: docker-compose.yml for development docker-compose.prod.yml for production","title":"File Naming"},{"location":"docker_compose/#version","text":"The version keyword tells docker-compose what version of configurations we will be using. This helps with issues of using configurations, keywords, etc. that may have been used in earlier versions, but have since changed. The first line of our docker-compose file should always be the version keyword. Example: If we wanted to use version 3.7 (latest at the time of documentation), we would use: version : \"3.7\"","title":"Version"},{"location":"docker_compose/#services","text":"Services with ** require additional configuration variables defined on this page Certain service parameters like Persistent Volumes and Shared Netowrks require additional configuration outside of the Services section. Services with this identifier will have their own section in this document. The services section is where we define all the containers that we want Docker to create and their configurations/relations. Each entry in the services section can have varying options and environment variables, which will be best defined on the core images' Dockerhub page. Below is a matrix of popular parameters for services and wether they're required for base functionality: Parameter Required Default Value Description container_name N random string Custom name for container; extremely useful to identify containers that belong to a stack. image Y Image name to be used for container. This is replaced by build when using Dockerfile to build a custom image. build Y Path to Dockerfile . This is replaced by image when using pre-built imaged from Dockerhub restart N never Define if container should attempt to restart on fail or docker boot (e.g. system restart) environment N Environment variables that should be passed into the container. ports N Port mapping from local system to container volumes N Docker Volume and local filesystem mapping into the container. Mapping a local directory to the container (e.g. ./src ) will ensure the files are always in-sync, in both directions, even while the container is running. networks N Shared network mapping between containers.** command N Custom command to be ran on container start/boot.** If we wanted to define: Node service that builds from our Dockerfile MySQL database from a pre-built image in Dockerhub Allow containers to communicate between each other Ensure our DB data persists even when containers are shutdown/destroyed Expose our Node service to our local system on port 80 we would use: version : \"3.7\" services : # our node application node_app : container_name : node_web # custom name for the container build : . # use Dockerfile located project root restart : unless-stopped # always restart, unless manually stopped environment : # define environment variables NODE_ENVIRONMENT : dev ports : # map ports from local:container - 80:8000 volumes : # map our source code into container workdir - ./src:/code networks : # use shared network between containers - node_db_network # mysql db for our node application node_db : container_name : node_db # custom name for the container image : mysql:latest # name:version of pre-built image restart : unless-stopped # always restart, unless manually stopped environment : # define environment variables MYSQL_ROOT_PASSWORD : root_pwd MYSQL_DATABASE : app_db MYSQL_USER : db_admin MYSQL_PASSWORD : db_admin_pwd volumes : # map persistent volume to DB store - node_db_volume:/var/lib/mysql networks : # use shared network between containers - node_db_network","title":"Services"},{"location":"docker_compose/#volumes","text":"In docker-compose, volumes can be used to: Create simple mappings of local code/config directories into the container, Map local directories to sync and retain data from within the container, Define Persistent Volumes that are named and kept even when containers are shutdown/recreated.","title":"Volumes"},{"location":"docker_compose/#simple-volumes","text":"In our examples we use a simple mapping to map our ./src directory into the container /code work directory, which ensures our source code is always in-sync, allowing us to make live edits to code while the containers are running. volumes : - ./src:/code","title":"Simple Volumes"},{"location":"docker_compose/#persistent-volumes","text":"For our database however, we do not have pre-existing data that we would want MySQL to use; instead, we would like to persist our data that is created once our application and database are up and running in Docker. To achieve this, we will map out network similar to a Simple Volume, but add a separate configuration outside of the Services section: # our usual services definition, where we # map our persistent volume to the container services : node_db : ... volumes : - node_db_network # persistent volume definition volumes : node_db_network :","title":"Persistent Volumes"},{"location":"docker_compose/#networks","text":"While we do define and expose ports on the individual containers to our local system, there are cases where we want certain containers to communicate with each other, and only each other. To achieve this, we create named networks and then define which containers have access to them. If we wanted to let our node_web container communicate with the database in node_db container, without exposing it to our local system, we would use: # our usual services definition, where we map our # named network to the containers that are allowed to use it services : node_web : ... networks : - node_db_network node_db : ... networks : - node_db_network # named network definition networks : node_db_network :","title":"Networks"},{"location":"docker_compose/#completed-example","text":"version : \"3.7\" services : # our node application node_app : container_name : node_web # custom name for the container build : . # use Dockerfile located project root restart : unless-stopped # always restart, unless manually stopped environment : # define environment variables NODE_ENVIRONMENT : dev ports : # map ports from local:container - 80:8000 volumes : # map our source code into container workdir - ./src:/code networks : # use shared network between containers - node_db_network # mysql db for our node application node_db : container_name : node_db # custom name for the container image : mysql:latest # name:version of pre-built image restart : unless-stopped # always restart, unless manually stopped environment : # define environment variables MYSQL_ROOT_PASSWORD : root_pwd MYSQL_DATABASE : app_db MYSQL_USER : db_admin MYSQL_PASSWORD : db_admin_pwd volumes : # map persistent volume to DB store - node_db_volume:/var/lib/mysql networks : # use shared network between containers - node_db_network # persistent volume definition volumes : node_db_network : # named network definition networks : node_db_network :","title":"Completed Example"},{"location":"dockerfile/","text":"Dockerfile Dockerfile is the core configuration file used to define custom built images/containers. If your stack is using pre-built images only (e.g. MySQL, node, etc.), then you would not need this file. However, if you're looking to run a Node server with your custom code, we would need to build a new image, mixing our code onto an existing Node image, and running that in a container. Keywords Below we will talk about the sections of a Dockerfile and how to use them. FROM Use Trusted Sources Only Core images should only be used from official sources such as Dockerhub , as images will contain base code that will run in your container. The first line of any Dockerfile should always define the core image we're starting from. Using an all caps FROM keyword, we can then specify the image, version, and variant, separating the image name and version/type with a semi-colo ( : ). Available image versions and variants should always be defined on the Image page of Dockerhub. Example: For a Node application, we would use a node:lts command to pull the latest LTS version of the Node image. FROM \"node:lts\" RUN The run command is used to define shell commands that should be executed at the time of compiling/building our custom image. These commands are typically used to: Adding missing/custom repositories for APT Install missing dependencies (e.g. missing mysql driver) Creating custom directories Example: if we wanted to create a custom directory for our application source code and run an APT update/upgrade to ensure the core image is up to date, we would use: RUN mkdir /code RUN apt update && apt upgrade -y WORKDIR The workdir keyword allows us to specify where our source code will live and most importantly, where to run our commands from, within the image. It is important to ensure our workdir directory exists before we specify this keyword, as the process will attempt to find and link to it. Example: If I wanted to define my wordir as /code , I would use: RUN mkdir /code WORKDIR /code COPY The copy command is used to copy files/directories into the image. Copying files into the image at compile, can be very helpful in setting up environments and installing pre-requisites that do not need to be ran every time a container is started. Example: If we wanted to copy our package.json config into the image and install the required Node packages, we would use: COPY package.json /code/ RUN yarn install EXPOSE Expose Not used when using Docker Compose While the expose command can be used to expose ports in the container, it is typically not used when a stack is defined using docker-compose.yml, as it compose will have it's own mapping that is better defined. The expose command can be used to expose container ports to the local environments. Example: if we wanted to expose port 8000 to our local machine, we would use: EXPOSE 8000 Completed Example # define core image:version/variant FROM \"node:lts-alpine\" # make our source code directory RUN mkdir /code # define our source code directory as the root for commands WORKDIR /code # copy our package.json config into the image COPY package.json /code/ # install our node package dependencies RUN yarn install # perform an initial copy of our source code into the image COPY ./src /code/ # production use: build our node source code RUN yarn build","title":"Dockerfile"},{"location":"dockerfile/#dockerfile","text":"Dockerfile is the core configuration file used to define custom built images/containers. If your stack is using pre-built images only (e.g. MySQL, node, etc.), then you would not need this file. However, if you're looking to run a Node server with your custom code, we would need to build a new image, mixing our code onto an existing Node image, and running that in a container.","title":"Dockerfile"},{"location":"dockerfile/#keywords","text":"Below we will talk about the sections of a Dockerfile and how to use them.","title":"Keywords"},{"location":"dockerfile/#from","text":"Use Trusted Sources Only Core images should only be used from official sources such as Dockerhub , as images will contain base code that will run in your container. The first line of any Dockerfile should always define the core image we're starting from. Using an all caps FROM keyword, we can then specify the image, version, and variant, separating the image name and version/type with a semi-colo ( : ). Available image versions and variants should always be defined on the Image page of Dockerhub. Example: For a Node application, we would use a node:lts command to pull the latest LTS version of the Node image. FROM \"node:lts\"","title":"FROM"},{"location":"dockerfile/#run","text":"The run command is used to define shell commands that should be executed at the time of compiling/building our custom image. These commands are typically used to: Adding missing/custom repositories for APT Install missing dependencies (e.g. missing mysql driver) Creating custom directories Example: if we wanted to create a custom directory for our application source code and run an APT update/upgrade to ensure the core image is up to date, we would use: RUN mkdir /code RUN apt update && apt upgrade -y","title":"RUN"},{"location":"dockerfile/#workdir","text":"The workdir keyword allows us to specify where our source code will live and most importantly, where to run our commands from, within the image. It is important to ensure our workdir directory exists before we specify this keyword, as the process will attempt to find and link to it. Example: If I wanted to define my wordir as /code , I would use: RUN mkdir /code WORKDIR /code","title":"WORKDIR"},{"location":"dockerfile/#copy","text":"The copy command is used to copy files/directories into the image. Copying files into the image at compile, can be very helpful in setting up environments and installing pre-requisites that do not need to be ran every time a container is started. Example: If we wanted to copy our package.json config into the image and install the required Node packages, we would use: COPY package.json /code/ RUN yarn install","title":"COPY"},{"location":"dockerfile/#expose","text":"Expose Not used when using Docker Compose While the expose command can be used to expose ports in the container, it is typically not used when a stack is defined using docker-compose.yml, as it compose will have it's own mapping that is better defined. The expose command can be used to expose container ports to the local environments. Example: if we wanted to expose port 8000 to our local machine, we would use: EXPOSE 8000","title":"EXPOSE"},{"location":"dockerfile/#completed-example","text":"# define core image:version/variant FROM \"node:lts-alpine\" # make our source code directory RUN mkdir /code # define our source code directory as the root for commands WORKDIR /code # copy our package.json config into the image COPY package.json /code/ # install our node package dependencies RUN yarn install # perform an initial copy of our source code into the image COPY ./src /code/ # production use: build our node source code RUN yarn build","title":"Completed Example"},{"location":"dockerignore/","text":"Dockerignore Similar to the popular .gitignore file, .dockerignore uses regex expressions to tell docker what files/directories to not sync or touch. If we had a Node application that has varying Node package dependencies, these dependencies should really be installed and maintained by our package manager (e.g. Yarn), and not copied/synced between local and container. Using .dockerignore , we can tell docker to disregard the node_modules directory, making our container mappings and sync a lot more efficient and faster. Example In the below example will configure docker to ignore: any and all files/folders in the node_modules directory any file with a extension of .log our environment file node_modules/ *.log .env","title":"Docker Ignore"},{"location":"dockerignore/#dockerignore","text":"Similar to the popular .gitignore file, .dockerignore uses regex expressions to tell docker what files/directories to not sync or touch. If we had a Node application that has varying Node package dependencies, these dependencies should really be installed and maintained by our package manager (e.g. Yarn), and not copied/synced between local and container. Using .dockerignore , we can tell docker to disregard the node_modules directory, making our container mappings and sync a lot more efficient and faster.","title":"Dockerignore"},{"location":"dockerignore/#example","text":"In the below example will configure docker to ignore: any and all files/folders in the node_modules directory any file with a extension of .log our environment file node_modules/ *.log .env","title":"Example"},{"location":"env_file/","text":"Environment FIle While docker compose does allow us to define variables within the docker-compose.yml file, this is not a secure or effective way to store sensitive information, as the docker-compose file should be committed and tracked in your version control, without concern of sharing passwords, usernames, etc. To simplify management of sensitive information between docker-compose configuration files, and allow for re-usability of variables, docker-compose recognizes .env files that live alongside the compose file. In the below example, we will define how to use .env files, version control them, and share the variables in docker-compose: Environment File Example While our actual .env file should never be committed or left in unsecured areas, it is very helpful to include an example of the actual file, showing what variables are required in an actual .env . Create a .env.example file and fill in your variable names, but not the values: MYSQL_ROOT_PASSWORD= MYSQL_DATABASE= MYSQL_USER= MYSQL_PASSWORD= Live Environment File MYSQL_ROOT_PASSWORD=root_pwd MYSQL_DATABASE=app_db MYSQL_USER=db_admin MYSQL_PASSWORD=dn_admin_pwd Use Env Variables in docker-compose Once we've defined our actual environment file, we can now call on them from our docker-compose files by using ${variable_name} . Example: services : node_db : container_name : node_db image : mysql:latest restart : unless-stopped environment : MYSQL_ROOT_PASSWORD : ${MYSQL_ROOT_PASSWORD} MYSQL_DATABASE : ${MYSQL_DATABASE} MYSQL_USER : ${MYSQL_USER} MYSQL_PASSWORD : ${MYSQL_PASSWORD}","title":"Env File"},{"location":"env_file/#environment-file","text":"While docker compose does allow us to define variables within the docker-compose.yml file, this is not a secure or effective way to store sensitive information, as the docker-compose file should be committed and tracked in your version control, without concern of sharing passwords, usernames, etc. To simplify management of sensitive information between docker-compose configuration files, and allow for re-usability of variables, docker-compose recognizes .env files that live alongside the compose file. In the below example, we will define how to use .env files, version control them, and share the variables in docker-compose:","title":"Environment FIle"},{"location":"env_file/#environment-file-example","text":"While our actual .env file should never be committed or left in unsecured areas, it is very helpful to include an example of the actual file, showing what variables are required in an actual .env . Create a .env.example file and fill in your variable names, but not the values: MYSQL_ROOT_PASSWORD= MYSQL_DATABASE= MYSQL_USER= MYSQL_PASSWORD=","title":"Environment File Example"},{"location":"env_file/#live-environment-file","text":"MYSQL_ROOT_PASSWORD=root_pwd MYSQL_DATABASE=app_db MYSQL_USER=db_admin MYSQL_PASSWORD=dn_admin_pwd","title":"Live Environment File"},{"location":"env_file/#use-env-variables-in-docker-compose","text":"Once we've defined our actual environment file, we can now call on them from our docker-compose files by using ${variable_name} . Example: services : node_db : container_name : node_db image : mysql:latest restart : unless-stopped environment : MYSQL_ROOT_PASSWORD : ${MYSQL_ROOT_PASSWORD} MYSQL_DATABASE : ${MYSQL_DATABASE} MYSQL_USER : ${MYSQL_USER} MYSQL_PASSWORD : ${MYSQL_PASSWORD}","title":"Use Env Variables in docker-compose"},{"location":"project_structure/","text":"Project Structure To simplify moving our code, configuration, and data between the container and local filesystem, it is best to designate few key directories. Sample structure: . \u251c\u2500\u2500 config \u2502 \u2514\u2500\u2500 nginx \u2502 \u2502 \u251c\u2500\u2500 certs \u2502 \u2502 \u2502 \u251c\u2500\u2500 nginx-selfsigned.crt \u2502 \u2502 \u2502 \u2514\u2500\u2500 nginx-selfsigned.key \u2502 \u2502 \u2514\u2500\u2500 d2compare.conf \u2502 \u2514\u2500\u2500 mysql \u2502 \u2514\u2500\u2500 my.conf \u251c\u2500\u2500 data \u2502 \u2514\u2500\u2500 <empty dir> \u251c\u2500\u2500 src \u2502 \u251c\u2500\u2500 app_code \u2502 \u2514\u2500\u2500 app.py \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 docker-compose.prod.yml \u2514\u2500\u2500 docker-compose.yml Config Application configuration files, that will be mapped into the container. These configurations do not have to be tied to your source code, and can be mapped into any destination within the container. E.g. mapping config/nginx/default.conf to /etc/nginx/sites-available Data Directories While it is recommended to keep data in persistent Docker volumes, there may be need to retain data outside of Docker. To store data externally, you can map a local directory to a data directory in the container, and have Docker mirror the container directory/files into your local filesystem. e.g. retaining a MySQL database, we would map data to /var/lib/mysql Src Application source code for the application. If applicable, all source code that runs in the application should live here. If there are multiple applications being brought up, they should be mapped into their own source directories. Docker Files Individual files that are used to configure our Docker stack, which should live at the root of the project directory. Dockerfile docker-compose.yml docker-compose.prod.yml .dockerignore","title":"Project Structure"},{"location":"project_structure/#project-structure","text":"To simplify moving our code, configuration, and data between the container and local filesystem, it is best to designate few key directories. Sample structure: . \u251c\u2500\u2500 config \u2502 \u2514\u2500\u2500 nginx \u2502 \u2502 \u251c\u2500\u2500 certs \u2502 \u2502 \u2502 \u251c\u2500\u2500 nginx-selfsigned.crt \u2502 \u2502 \u2502 \u2514\u2500\u2500 nginx-selfsigned.key \u2502 \u2502 \u2514\u2500\u2500 d2compare.conf \u2502 \u2514\u2500\u2500 mysql \u2502 \u2514\u2500\u2500 my.conf \u251c\u2500\u2500 data \u2502 \u2514\u2500\u2500 <empty dir> \u251c\u2500\u2500 src \u2502 \u251c\u2500\u2500 app_code \u2502 \u2514\u2500\u2500 app.py \u251c\u2500\u2500 Dockerfile \u251c\u2500\u2500 docker-compose.prod.yml \u2514\u2500\u2500 docker-compose.yml","title":"Project Structure"},{"location":"project_structure/#config","text":"Application configuration files, that will be mapped into the container. These configurations do not have to be tied to your source code, and can be mapped into any destination within the container. E.g. mapping config/nginx/default.conf to /etc/nginx/sites-available","title":"Config"},{"location":"project_structure/#data-directories","text":"While it is recommended to keep data in persistent Docker volumes, there may be need to retain data outside of Docker. To store data externally, you can map a local directory to a data directory in the container, and have Docker mirror the container directory/files into your local filesystem. e.g. retaining a MySQL database, we would map data to /var/lib/mysql","title":"Data Directories"},{"location":"project_structure/#src","text":"Application source code for the application. If applicable, all source code that runs in the application should live here. If there are multiple applications being brought up, they should be mapped into their own source directories.","title":"Src"},{"location":"project_structure/#docker-files","text":"Individual files that are used to configure our Docker stack, which should live at the root of the project directory. Dockerfile docker-compose.yml docker-compose.prod.yml .dockerignore","title":"Docker Files"}]}